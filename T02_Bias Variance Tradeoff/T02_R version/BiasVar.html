<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>T02: Bias-variance trade-off</title>
    <meta charset="utf-8" />
    <meta name="author" content="HUANG Xinrui" />
    <meta name="date" content="2023-09-19" />
    <script src="libs/header-attrs-2.16/header-attrs.js"></script>
    <link href="libs/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.2.6/panelset.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# T02: Bias-variance trade-off
]
.subtitle[
## MATH 4432 Statistical Machine Learning
]
.author[
### HUANG Xinrui
]
.institute[
### MATH, HKUST
]
.date[
### 2023-09-19
]

---










class: inverse, center, middle

# Let's start by recalling what we have learned in class!

---

## Biasâ€“variance decomposition of squared error

When fitting a model, we want to minimize
`$$\mathbb{E}_{\mathcal{D}}\left[\left(f(X)-\hat{f}(X;\mathcal{D})\right)^2\right]$$`
w.r.t `\(\hat{f}\)`.

If we add and substract `\(\mathbb{E}_{\mathcal{D}}\left[\hat{f}(X;\mathcal{D})\right]\)` inside the brackets, we have
$$
`\begin{aligned}
    &amp;\mathbb{E}_{\mathcal{D}}\left[\left(f(X)-\hat{f}(X;\mathcal{D})\right)^2\right] \\
    = &amp;\underbrace{\left(f(X) - \mathbb{E}_{\mathcal{D}}\left[\hat{f}(X;\mathcal{D})\right]\right)^2}_{Bias^2} + \underbrace{\mathbb{E}_{\mathcal{D}}\left[\left(\mathbb{E}_{\mathcal{D}}\left[\hat{f}(X;\mathcal{D})\right] - \hat{f}(X;\mathcal{D})\right)^2\right]}_{Variance}
\end{aligned}`
$$

---

## Two sources of error

- **Bias** from erroneous assumptions in the learning algorithm.

  High bias `\(\longrightarrow\)` **underfitting**.
  
- **Variance** from sensitivity to small fluctuations in the training set.

  High variance `\(\longrightarrow\)` **overfitting**.

&lt;img src="bias_variance.png" width="95%" style="display: block; margin: auto;" /&gt;

---
class: inverse, center, middle

# A toy example

---

## Problem setting

.panelset[

.panel[.panel-name[Setting]

- Suppose we know the ground truth of `\(f(\cdot):f(x)=\sin(2\pi x)\)`

- Now given `\(\{x_n\}_{n=1}^N\)`, we have a set of observations `\(\mathcal{D}=\{(x_n,y_n)\}_{n=1}^N\)` with `$$y_n=f(x_n) + \epsilon_n,$$` where `\(\epsilon_n \stackrel{\text { i.i.d. }}{\sim} \mathcal{N}(0,1)\)` is random noise.

.panel[.panel-name[Code]

```r
set.seed(123)

# Ground truth
x &lt;- seq(0, 1, length.out = 100)
y &lt;- sin(2 * pi * x)
# Observed data
N &lt;- 50 # Sample size
X &lt;- runif(N, 0, 1)
y0 &lt;- sin(2 * pi * X)
y_obs &lt;- y0 + rnorm(N, mean = 0, sd = 1) # Add noise

ggplot(data = NULL) +
  geom_line(aes(x = x, y = y), color = "red", size = 2) +
  geom_point(aes(x = X, y = y_obs), size = 5) +
  theme(
    text = element_text(size = 18),
    axis.text.y = element_text(size = 18),
    axis.text.x = element_text(size = 18)
    )
```



]

.panel[.panel-name[Plot]

&lt;img src="BiasVar_files/figure-html/unnamed-chunk-4-1.png" width="50%" style="display: block; margin: auto;" /&gt;

]

]

.panel[.panel-name[Fit the model]

For a given `\(\mathcal{D}\)`, we can fit a model `\(\hat{f}(X;\mathcal{D})\)`, e.g., linear regression.

.panel[.panel-name[Code]

```r
set.seed(123)

ggplot(data = NULL) +
  geom_line(aes(x = x, y = y), color = "red", size = 2) +
  geom_point(aes(x = X, y = y_obs), size = 5) +
  geom_smooth(aes(x = X, y = y_obs), method = "lm") + # Linear regression
  theme(legend.position = "none") +
  theme(
    text = element_text(size = 18),
    axis.text.y = element_text(size = 18),
    axis.text.x = element_text(size = 18)
    )
```



]

.panel[.panel-name[Plot]

```
#&gt; `geom_smooth()` using formula = 'y ~ x'
```

&lt;img src="BiasVar_files/figure-html/unnamed-chunk-5-1.png" width="50%" style="display: block; margin: auto;" /&gt;

]

]

]

---

## Smoothing spline &lt;sup&gt;*&lt;/sup&gt;

Assume `\(f(\cdot)\)` is some unknown **smooth** function, to estimate `\(f(\cdot)\)`, a smoothing spline minimizes the penalized least squares functional
`$$f_{\lambda} = \min_{f \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^n \left(y_i-f\left(x_i\right)\right)^2 + \lambda J_m(f),$$`
where `\(J_m(f)=\int\left|f^{(m)}(z)\right|^2 d z\)` is a penalty term that quantifies the lack of parsimony of the function estimate, and `\(\lambda&gt;0\)` is the smoothing parameter that controls the influence of the penalty.

Note that `\(f^{(m)}(\cdot)\)` denotes the `\(m\)`-th derivative of `\(f(\cdot)\)`, and `\(\mathcal{H}=\left\{f: J_m(f)&lt;\infty\right\}\)` is the space of functions with square integrable `\(m\)`-th derivative.

.footnote[
[*] See [Wikipedia](https://en.wikipedia.org/wiki/Smoothing_spline#:~:text=Smoothing%20splines%20are%20function%20estimates,means%20for%20smoothing%20noisy%20data.) or [Smoothing Spline Regression in R](http://users.stat.umn.edu/~helwig/notes/smooth-spline-notes.html) for more details if you are interested. However, this is not the point of this course!
]

---

## Smoothing parameter

.panelset[

.panel[.panel-name[Smoothing parameter influence]

- As `\(\lambda \rightarrow 0\)` the penalty has less influence on the penalized least squares functional. So, for very small values of `\(\lambda\)`, the function estimate `\(f_\lambda\)` essentially minimizes the residual sum of squares.

- As `\(\lambda \rightarrow \infty\)` the penalty has more influence on the penalized least squares functional. So, for very large values of `\(\lambda\)`, the function estimate `\(f_\lambda\)` is essentially constrained to have a zero penalty, i.e., `\(J_m\left(f_\lambda\right) \approx 0\)`.

- As `\(\lambda\)` increases from 0 to `\(\infty\)`, the function estimate `\(f_\lambda\)` is forced to be smoother with respect to the penalty functional `\(J_m(\cdot)\)`. The goal is to find the `\(\lambda\)` that produces the "correct" degree of smoothness for the function estimate.

.panel[.panel-name[Code]

```r
library(ggformula)

ggplot(data = NULL, aes(x = X, y = y_obs)) +
  geom_point(size = 5) +
  geom_spline(aes(x = X, y = y_obs), spar = 1e-2, colour = "red", size = 2) + # Small lambda
  geom_spline(aes(x = X, y = y_obs), cv = TRUE, colour = "blue", size = 2) + # Cross-validation
  geom_spline(aes(x = X, y = y_obs), spar = 2, colour = "green", size = 2) + # Large lambda
  xlab("x") +
  ylab("y") +
  ggtitle("Smoothing spline regression") +
  theme(
    text = element_text(size = 18),
    axis.text.y = element_text(size = 18),
    axis.text.x = element_text(size = 18)
    )
```



]

.panel[.panel-name[Plot]

&lt;img src="BiasVar_files/figure-html/unnamed-chunk-6-1.png" width="50%" style="display: block; margin: auto;" /&gt;

]

]

]

---

## How do we evaluate the bias-variance trade-off in this example?

We need `\(\mathbb{E}_{\mathcal{D}}(\hat{f}(X;\mathcal{D}))\)`, therefore, we need to have multiple datasets.

- For `\(l=1,...,L\)`,
    - generate `\(\mathcal{D}^l=\{(x_n,y^l_n)\}_{n=1}^N\)`, where `\(y^l_n=f(x_n) + \epsilon^l_n\)`
    - fit the `\(l\)`-th model `\(\hat{f}(X^l;\mathcal{D}^l)\)` and denote the predicted value as `\(y^l(x_n)=\hat{f}(x_n;\mathcal{D}^l)\)`
    
  **Note that `\(f(x_n)\)` is fixed across `\(l\)` while the observed values `\(y^l_n\)` are varying due to random noise `\(\epsilon_n^l\)`.**

- Estimate `\(\mathbb{E}_{\mathcal{D}}[\hat{f}(X;\mathcal{D})]\)` by `\(\bar{y}(x)=\frac{1}{L}\sum_{l=1}^Ly^l(x)\)`

- Compute squared bias: `\(\frac{1}{N}\sum_{n=1}^N \left( \bar{y}(x_n)-f(x_n) \right)^2\)`

- Compute variance: `\(\frac{1}{N}\sum_{n=1}^N\frac{1}{L}\sum_{l=1}^L \left( y^l(x_n)-\bar{y}(x_n) \right)^2\)`

---

## Experiments

.panelset[

.panel[.panel-name[Experiments setting]

We take the above example with `\(N = 20\)`, `\(L = 500\)` and use smoothing spline regression with `\(\lambda \in [1 \times 10^{-6}, 10]\)`.


```r
set.seed(123)

trial &lt;- 500 # Number of experiment trials
N &lt;- 20 # Number of samples for each trial
lambda_list &lt;- exp(seq(log(1e-6), log(10), length.out = 100)) # Parameter list
model_list &lt;- list() # Model list
biasSQ &lt;- variance &lt;- vector(mode = "numeric", length = length(lambda_list)) # Bias and variance

X &lt;- runif(N, 0, 1) # Predictor
y0 &lt;- sin(2 * pi * X) # True values of y
y_mat = matrix(0, nrow = N, ncol = trial) # Store the generated response variables
for(j in 1 : trial){
  y_mat[, j] = y0 + rnorm(N, mean = 0, sd = 1) # Add noise; each column contains a dataset 
}
```

]

.panel[.panel-name[Implementation]


```r
for(i in 1 : length(lambda_list)){
  model_list_i &lt;- list()
  y_hat &lt;- matrix(0, nrow = N, ncol = trial) # Predicted values
  
  for(j in 1 : trial){
    y &lt;- y_mat[, j]
    fit_ss &lt;- smooth.spline(x = X, y = y, lambda = lambda_list[i]) # Smoothing spline regression
    model_list_i &lt;- c(model_list_i, list(fit_ss)) # Save the model for the current training dataset
    y_hat[, j] &lt;- predict(fit_ss, X)$y # Predicted values
  }
  
  model_list &lt;- c(model_list, list(model_list_i)) # Save the model list for the current parameter
  y_bar &lt;- rowMeans(y_hat) # Mean of predicted values, E(f^hat)
  biasSQ[i] &lt;- mean((y0 - y_bar)^2)  # Bias square, E[ (f - E(f^hat))^2 ]
  variance[i] &lt;- mean((y_hat - y_bar)^2)   # Variance, E[ (E(f^hat) - f^hat)^2 ]
  }
```

]

]

---

## Visualization

.panelset[

.panel[.panel-name[Bias and variance]

Let's first take a look at how the two sources of error change as the parameter `\(\lambda\)` changes.

.panel[.panel-name[Code]

```r
ggplot(data = NULL) +
  geom_line(aes(x = log(lambda_list), y = biasSQ, color = "biasSQ"), size = 2) +
  geom_line(aes(x = log(lambda_list), y = variance, color = "variance"), size = 2) +
  geom_line(aes(x = log(lambda_list), y = biasSQ + variance, color = "biasSQ + variance"), size = 2) +
  geom_vline(xintercept = log(lambda_list)[which.min(biasSQ + variance)], linetype = "longdash", size = 2) + # Minimal error
  xlab("log(lambda)") +
  ylab("Error") +
  scale_color_manual(name = "Sources of error",
                     breaks = c("biasSQ", "variance", "biasSQ + variance"),
                     values = c("biasSQ" = "red", "variance" = "blue", "biasSQ + variance" = "green")) +
  theme(
    text = element_text(size = 18),
    axis.text.y = element_text(size = 18),
    axis.text.x = element_text(size = 18),
    legend.title = element_text(size = 15),
    legend.text = element_text(size = 15),
    legend.position = "bottom"
    )
```



]

.panel[.panel-name[Plot]

&lt;img src="BiasVar_files/figure-html/unnamed-chunk-9-1.png" width="50%" style="display: block; margin: auto;" /&gt;

]

]

]

---

## More details

.panelset[

.panel[.panel-name[More details]

Then we chose three different values for the parameter `\(\lambda\)` (too small, suitable, too large) and visualize for more performance details.

.panel[.panel-name[Code]

```r
par(mfrow = c(3, 2))

N &lt;- 100
lambda_idx_list &lt;- c(1, which.min(biasSQ + variance), 100)
x &lt;- seq(0, 1, length.out = N)
y0 &lt;- sin(2 * pi * x)

for(lambda_idx in lambda_idx_list){
  plot(x, y0, col = "green", type = "l", ylim = c(-3, 3), ylab = "y", cex = 2, cex.axis = 2, cex.lab = 2)
  
  y_hat &lt;- matrix(0, nrow = N, ncol = 20)
  for(j in 1 : 20){
    y_hat[, j] &lt;- predict(model_list[[lambda_idx]][[j]], x)$y
    lines(x, y_hat[, j], col = "red", ylim = c(-3, 3))
    }
  
  plot(x, y0, col = "green", type = "l", ylim = c(-3, 3), ylab = "y", cex = 2, cex.axis = 2, cex.lab = 2)
  lines(x, rowMeans(y_hat), col = "red", ylim = c(-3, 3))
  }
```



]

.panel[.panel-name[Plot]

&lt;img src="BiasVar_files/figure-html/unnamed-chunk-10-1.png" width="50%" style="display: block; margin: auto;" /&gt;

]
]

]

---
class: inverse, center, middle

# Thank you!

Slides created via Yihui Xie's R package [**xaringan**](https://github.com/yihui/xaringan).

Theme customized via Garrick Aden-Buie's R package [**xaringanthemer**](https://github.com/gadenbuie/xaringanthemer).

Tabbed panels created via Garrick Aden-Buie's R package [**xaringanExtra**](https://github.com/gadenbuie/xaringanExtra/).

The chakra comes from [remark.js](https://remarkjs.com), [**knitr**](https://yihui.org/knitr/), and [R Markdown](https://rmarkdown.rstudio.com).
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
